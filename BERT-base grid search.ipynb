{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Institution name</th>\n",
       "      <th>Main panel_x</th>\n",
       "      <th>Unit of assessment number_x</th>\n",
       "      <th>Unit of assessment name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary impact type</th>\n",
       "      <th>Countries</th>\n",
       "      <th>Formal partners</th>\n",
       "      <th>Funding programmes</th>\n",
       "      <th>Global research identifiers</th>\n",
       "      <th>...</th>\n",
       "      <th>1. Summary of the impact</th>\n",
       "      <th>2. Underpinning research</th>\n",
       "      <th>3. References to the research</th>\n",
       "      <th>4. Details of the impact</th>\n",
       "      <th>5. Sources to corroborate the impact</th>\n",
       "      <th>Weighted Avg</th>\n",
       "      <th>Variance</th>\n",
       "      <th>5 Class Labels</th>\n",
       "      <th>9 Class Labels</th>\n",
       "      <th>cleaned text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>University of East London</td>\n",
       "      <td>C</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>?Go Home?; Shaping the practice, policy and un...</td>\n",
       "      <td>Societal</td>\n",
       "      <td>[Italy];[Sweden];[Netherlands ];[Ireland];[Uni...</td>\n",
       "      <td>[University of Eastern Finland];[Middle East T...</td>\n",
       "      <td>[European Commission under the 7th Framework P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>###  1. Summary of the impact \\(indicative max...</td>\n",
       "      <td>###  2. Underpinning research \\(indicative max...</td>\n",
       "      <td>###  3. References to the research \\(indicativ...</td>\n",
       "      <td>###  4. Details of the impact \\(indicative max...</td>\n",
       "      <td>###  5. Sources to corroborate the impact \\(in...</td>\n",
       "      <td>2.667</td>\n",
       "      <td>0.556111</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Since 2013, CMRB findings have been used to su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University of Lincoln</td>\n",
       "      <td>A</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Agriculture, Food and Veterinary Sciences</td>\n",
       "      <td>Agricultural Robotics: Data Driven Technologie...</td>\n",
       "      <td>Technological</td>\n",
       "      <td>[England];[Norway];[USA]</td>\n",
       "      <td>[SAGA Robotics];[Garford Farms Limited];[Berry...</td>\n",
       "      <td>[N/A]</td>\n",
       "      <td>[grid.423443.6];[grid.418100.c]</td>\n",
       "      <td>...</td>\n",
       "      <td>###  1. Summary of the impact \\(indicative max...</td>\n",
       "      <td>###  2. Underpinning research \\(indicative max...</td>\n",
       "      <td>###  3. References to the research \\(indicativ...</td>\n",
       "      <td>###  4. Details of the impact \\(indicative max...</td>\n",
       "      <td>###  5. Sources to corroborate the impact \\(in...</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>UoL’s innovative agri-food research has underp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Psychology, Psychiatry and Neuroscience</td>\n",
       "      <td>Alleviating the societal and economic burden o...</td>\n",
       "      <td>Health</td>\n",
       "      <td>[England];[China];[USA];[Canada];[Belgium];[Fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[grid.14105.31];[grid.473755.7];[grid.52788.30]</td>\n",
       "      <td>...</td>\n",
       "      <td>**1. Summary of the impact** \\(indicative maxi...</td>\n",
       "      <td>**2. Underpinning research \\(indicative maximu...</td>\n",
       "      <td>**3. References to the research** \\(indicative...</td>\n",
       "      <td>**4. Details of the impact** \\(indicative maxi...</td>\n",
       "      <td>**5. Sources to corroborate the impact** \\(ind...</td>\n",
       "      <td>3.667</td>\n",
       "      <td>0.444111</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>The Cambridge University research has had a si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>A</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Agriculture, Food and Veterinary Sciences</td>\n",
       "      <td>Changes in policy for the control of bovine tu...</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>[England];[Wales];[Ethiopia]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[grid.418100.c];[grid.434257.3];[grid.433527.4...</td>\n",
       "      <td>...</td>\n",
       "      <td>###  1. Summary of the impact \\(indicative max...</td>\n",
       "      <td>###  2. Underpinning research \\(indicative max...</td>\n",
       "      <td>###  3. References to the research \\(indicativ...</td>\n",
       "      <td>###  4. Details of the impact \\(indicative max...</td>\n",
       "      <td>###  5. Sources to corroborate the impact \\(in...</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Bovine tuberculosis is a substantial economic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>University of Derby</td>\n",
       "      <td>C</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Social Work and Social Policy</td>\n",
       "      <td>Changing Police and Investigative Practice thr...</td>\n",
       "      <td>Legal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>###  1. Summary of the impact \\(indicative max...</td>\n",
       "      <td>###  2. Underpinning research \\(indicative max...</td>\n",
       "      <td>###  3. References to the research \\(indicativ...</td>\n",
       "      <td>###  4. Details of the impact \\(indicative max...</td>\n",
       "      <td>###  5. Sources to corroborate the impact \\(in...</td>\n",
       "      <td>2.333</td>\n",
       "      <td>1.554111</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>The impact of Bull's work is on: (1) internati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6293</th>\n",
       "      <td>School of Oriental and African Studies</td>\n",
       "      <td>C</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Anthropology and Development Studies</td>\n",
       "      <td>Transforming Working Practices in the House of...</td>\n",
       "      <td>Societal</td>\n",
       "      <td>[UK, Europe, Asia, Africa]</td>\n",
       "      <td>[Hansard Society];[Leeds University];[Jawaharl...</td>\n",
       "      <td>[ERC-2018-ADG - ERC Advanced Grant];[Research ...</td>\n",
       "      <td>[grid.452896.4];[grid.452966.a];[grid.426413.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>**1. Summary of the impact** \\(indicative maxi...</td>\n",
       "      <td>**2. Underpinning research** \\(indicative maxi...</td>\n",
       "      <td>**3. References to the research** \\(indicative...</td>\n",
       "      <td>**4. Details of the impact** \\(indicative maxi...</td>\n",
       "      <td>**5. Sources to corroborate the impact** \\(ind...</td>\n",
       "      <td>3.750</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Many scholars, MPs, civil servants and Commons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>Nottingham Trent University</td>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Allied Health Professions, Dentistry, Nursing ...</td>\n",
       "      <td>Translating biomedical nanotechnology into com...</td>\n",
       "      <td>Technological</td>\n",
       "      <td>[Algeria, Argentina, Australia, Austria, Azore...</td>\n",
       "      <td>[Royal Free Hospital];[AHDB];[Agrimin Ltd]</td>\n",
       "      <td>[Anthony Nolan PhD studentship];[BBSRC CASE st...</td>\n",
       "      <td>[grid.426412.7];[grid.418100.c];[grid.423443.6]</td>\n",
       "      <td>...</td>\n",
       "      <td>**1. Summary of the impact** \\n\\n NTU?s patent...</td>\n",
       "      <td>**2. Underpinning research** \\n\\n NTU iron oxi...</td>\n",
       "      <td>**3. References to the research** \\(NTU resear...</td>\n",
       "      <td>**4. Details of the impact** \\n\\n **Impacts on...</td>\n",
       "      <td>**5. Sources to corroborate the impact** \\(\\* ...</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Pharm2Farm Ltd. has commercialised NTU's paten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>King's College London</td>\n",
       "      <td>D</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Modern Languages and Linguistics</td>\n",
       "      <td>Translating for Performance: Spanish- and Port...</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>[UK]</td>\n",
       "      <td>[Monica Vasconcelos]</td>\n",
       "      <td>[Grants for the Arts]</td>\n",
       "      <td>[grid.422906.b]</td>\n",
       "      <td>...</td>\n",
       "      <td>###  1. Summary of the impact \\n\\n King?s rese...</td>\n",
       "      <td>###  2. Underpinning research \\n\\n Our researc...</td>\n",
       "      <td>###  3. References to the research \\n\\n 1. Boy...</td>\n",
       "      <td>###  4. Details of the impact \\n\\n King?s rese...</td>\n",
       "      <td>###  5. Sources to corroborate the impact \\n\\n...</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>King’s research has responded practically to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310</th>\n",
       "      <td>University of Glasgow</td>\n",
       "      <td>B</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Translating the significance of gravitational ...</td>\n",
       "      <td>Societal</td>\n",
       "      <td>[UK];[Worldwide]</td>\n",
       "      <td>[LIGO Scientific Collaboration];[National Scie...</td>\n",
       "      <td>[1) PARTICLE PHYSICS EXPERIMENT (PPE) CONSOLID...</td>\n",
       "      <td>[grid.14467.30 (all grants)]</td>\n",
       "      <td>...</td>\n",
       "      <td>**1. Summary of the impact** \\n\\n September 20...</td>\n",
       "      <td>**2. Underpinning research** \\n\\n The detectio...</td>\n",
       "      <td>**3. References to the research** \\(\\* = best ...</td>\n",
       "      <td>**4. Details of the impact** \\n\\n **UofG resea...</td>\n",
       "      <td>**5. Sources to corroborate the impact** \\n\\n1...</td>\n",
       "      <td>3.200</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>UofG’s Institute for Gravitational Research (I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>The University of Essex</td>\n",
       "      <td>B</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Computer Science and Informatics</td>\n",
       "      <td>UltraSoC Technologies - On-chip debug, monitor...</td>\n",
       "      <td>Technological</td>\n",
       "      <td>[United Kingdom]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[grid.421091.f]</td>\n",
       "      <td>...</td>\n",
       "      <td>###  1. Summary of the impact \\n\\n Essex resea...</td>\n",
       "      <td>###  2. Underpinning research \\n\\n Since the e...</td>\n",
       "      <td>###  3. References to the research \\[can be su...</td>\n",
       "      <td>###  4. Details of the impact \\n\\n UltraSoC wa...</td>\n",
       "      <td>###  5. Sources to corroborate the impact \\n\\n...</td>\n",
       "      <td>3.833</td>\n",
       "      <td>0.139111</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>UltraSoC was spun out to commercialise the deb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6286 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Institution name Main panel_x  \\\n",
       "0                  University of East London            C   \n",
       "1                      University of Lincoln            A   \n",
       "2                    University of Cambridge            A   \n",
       "3                    University of Cambridge            A   \n",
       "4                        University of Derby            C   \n",
       "...                                      ...          ...   \n",
       "6293  School of Oriental and African Studies            C   \n",
       "6300             Nottingham Trent University            A   \n",
       "6305                   King's College London            D   \n",
       "6310                   University of Glasgow            B   \n",
       "6333                 The University of Essex            B   \n",
       "\n",
       "      Unit of assessment number_x  \\\n",
       "0                            21.0   \n",
       "1                             6.0   \n",
       "2                             4.0   \n",
       "3                             6.0   \n",
       "4                            20.0   \n",
       "...                           ...   \n",
       "6293                         22.0   \n",
       "6300                          3.0   \n",
       "6305                         26.0   \n",
       "6310                          9.0   \n",
       "6333                         11.0   \n",
       "\n",
       "                                Unit of assessment name  \\\n",
       "0                                             Sociology   \n",
       "1             Agriculture, Food and Veterinary Sciences   \n",
       "2               Psychology, Psychiatry and Neuroscience   \n",
       "3             Agriculture, Food and Veterinary Sciences   \n",
       "4                         Social Work and Social Policy   \n",
       "...                                                 ...   \n",
       "6293               Anthropology and Development Studies   \n",
       "6300  Allied Health Professions, Dentistry, Nursing ...   \n",
       "6305                   Modern Languages and Linguistics   \n",
       "6310                                            Physics   \n",
       "6333                   Computer Science and Informatics   \n",
       "\n",
       "                                                  Title Summary impact type  \\\n",
       "0     ?Go Home?; Shaping the practice, policy and un...            Societal   \n",
       "1     Agricultural Robotics: Data Driven Technologie...       Technological   \n",
       "2     Alleviating the societal and economic burden o...              Health   \n",
       "3     Changes in policy for the control of bovine tu...       Environmental   \n",
       "4     Changing Police and Investigative Practice thr...               Legal   \n",
       "...                                                 ...                 ...   \n",
       "6293  Transforming Working Practices in the House of...            Societal   \n",
       "6300  Translating biomedical nanotechnology into com...       Technological   \n",
       "6305  Translating for Performance: Spanish- and Port...            Cultural   \n",
       "6310  Translating the significance of gravitational ...            Societal   \n",
       "6333  UltraSoC Technologies - On-chip debug, monitor...       Technological   \n",
       "\n",
       "                                              Countries  \\\n",
       "0     [Italy];[Sweden];[Netherlands ];[Ireland];[Uni...   \n",
       "1                              [England];[Norway];[USA]   \n",
       "2     [England];[China];[USA];[Canada];[Belgium];[Fr...   \n",
       "3                          [England];[Wales];[Ethiopia]   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "6293                         [UK, Europe, Asia, Africa]   \n",
       "6300  [Algeria, Argentina, Australia, Austria, Azore...   \n",
       "6305                                               [UK]   \n",
       "6310                                   [UK];[Worldwide]   \n",
       "6333                                   [United Kingdom]   \n",
       "\n",
       "                                        Formal partners  \\\n",
       "0     [University of Eastern Finland];[Middle East T...   \n",
       "1     [SAGA Robotics];[Garford Farms Limited];[Berry...   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "6293  [Hansard Society];[Leeds University];[Jawaharl...   \n",
       "6300         [Royal Free Hospital];[AHDB];[Agrimin Ltd]   \n",
       "6305                               [Monica Vasconcelos]   \n",
       "6310  [LIGO Scientific Collaboration];[National Scie...   \n",
       "6333                                                NaN   \n",
       "\n",
       "                                     Funding programmes  \\\n",
       "0     [European Commission under the 7th Framework P...   \n",
       "1                                                 [N/A]   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "6293  [ERC-2018-ADG - ERC Advanced Grant];[Research ...   \n",
       "6300  [Anthony Nolan PhD studentship];[BBSRC CASE st...   \n",
       "6305                              [Grants for the Arts]   \n",
       "6310  [1) PARTICLE PHYSICS EXPERIMENT (PPE) CONSOLID...   \n",
       "6333                                                NaN   \n",
       "\n",
       "                            Global research identifiers  ...  \\\n",
       "0                                                   NaN  ...   \n",
       "1                       [grid.423443.6];[grid.418100.c]  ...   \n",
       "2       [grid.14105.31];[grid.473755.7];[grid.52788.30]  ...   \n",
       "3     [grid.418100.c];[grid.434257.3];[grid.433527.4...  ...   \n",
       "4                                                   NaN  ...   \n",
       "...                                                 ...  ...   \n",
       "6293  [grid.452896.4];[grid.452966.a];[grid.426413.6...  ...   \n",
       "6300    [grid.426412.7];[grid.418100.c];[grid.423443.6]  ...   \n",
       "6305                                    [grid.422906.b]  ...   \n",
       "6310                       [grid.14467.30 (all grants)]  ...   \n",
       "6333                                    [grid.421091.f]  ...   \n",
       "\n",
       "                               1. Summary of the impact  \\\n",
       "0     ###  1. Summary of the impact \\(indicative max...   \n",
       "1     ###  1. Summary of the impact \\(indicative max...   \n",
       "2     **1. Summary of the impact** \\(indicative maxi...   \n",
       "3     ###  1. Summary of the impact \\(indicative max...   \n",
       "4     ###  1. Summary of the impact \\(indicative max...   \n",
       "...                                                 ...   \n",
       "6293  **1. Summary of the impact** \\(indicative maxi...   \n",
       "6300  **1. Summary of the impact** \\n\\n NTU?s patent...   \n",
       "6305  ###  1. Summary of the impact \\n\\n King?s rese...   \n",
       "6310  **1. Summary of the impact** \\n\\n September 20...   \n",
       "6333  ###  1. Summary of the impact \\n\\n Essex resea...   \n",
       "\n",
       "                               2. Underpinning research  \\\n",
       "0     ###  2. Underpinning research \\(indicative max...   \n",
       "1     ###  2. Underpinning research \\(indicative max...   \n",
       "2     **2. Underpinning research \\(indicative maximu...   \n",
       "3     ###  2. Underpinning research \\(indicative max...   \n",
       "4     ###  2. Underpinning research \\(indicative max...   \n",
       "...                                                 ...   \n",
       "6293  **2. Underpinning research** \\(indicative maxi...   \n",
       "6300  **2. Underpinning research** \\n\\n NTU iron oxi...   \n",
       "6305  ###  2. Underpinning research \\n\\n Our researc...   \n",
       "6310  **2. Underpinning research** \\n\\n The detectio...   \n",
       "6333  ###  2. Underpinning research \\n\\n Since the e...   \n",
       "\n",
       "                          3. References to the research  \\\n",
       "0     ###  3. References to the research \\(indicativ...   \n",
       "1     ###  3. References to the research \\(indicativ...   \n",
       "2     **3. References to the research** \\(indicative...   \n",
       "3     ###  3. References to the research \\(indicativ...   \n",
       "4     ###  3. References to the research \\(indicativ...   \n",
       "...                                                 ...   \n",
       "6293  **3. References to the research** \\(indicative...   \n",
       "6300  **3. References to the research** \\(NTU resear...   \n",
       "6305  ###  3. References to the research \\n\\n 1. Boy...   \n",
       "6310  **3. References to the research** \\(\\* = best ...   \n",
       "6333  ###  3. References to the research \\[can be su...   \n",
       "\n",
       "                               4. Details of the impact  \\\n",
       "0     ###  4. Details of the impact \\(indicative max...   \n",
       "1     ###  4. Details of the impact \\(indicative max...   \n",
       "2     **4. Details of the impact** \\(indicative maxi...   \n",
       "3     ###  4. Details of the impact \\(indicative max...   \n",
       "4     ###  4. Details of the impact \\(indicative max...   \n",
       "...                                                 ...   \n",
       "6293  **4. Details of the impact** \\(indicative maxi...   \n",
       "6300  **4. Details of the impact** \\n\\n **Impacts on...   \n",
       "6305  ###  4. Details of the impact \\n\\n King?s rese...   \n",
       "6310  **4. Details of the impact** \\n\\n **UofG resea...   \n",
       "6333  ###  4. Details of the impact \\n\\n UltraSoC wa...   \n",
       "\n",
       "                   5. Sources to corroborate the impact Weighted Avg  \\\n",
       "0     ###  5. Sources to corroborate the impact \\(in...        2.667   \n",
       "1     ###  5. Sources to corroborate the impact \\(in...        3.000   \n",
       "2     **5. Sources to corroborate the impact** \\(ind...        3.667   \n",
       "3     ###  5. Sources to corroborate the impact \\(in...        3.500   \n",
       "4     ###  5. Sources to corroborate the impact \\(in...        2.333   \n",
       "...                                                 ...          ...   \n",
       "6293  **5. Sources to corroborate the impact** \\(ind...        3.750   \n",
       "6300  **5. Sources to corroborate the impact** \\(\\* ...        3.000   \n",
       "6305  ###  5. Sources to corroborate the impact \\n\\n...        3.000   \n",
       "6310  **5. Sources to corroborate the impact** \\n\\n1...        3.200   \n",
       "6333  ###  5. Sources to corroborate the impact \\n\\n...        3.833   \n",
       "\n",
       "      Variance 5 Class Labels  9 Class Labels  \\\n",
       "0     0.556111              3               5   \n",
       "1     0.666000              3               6   \n",
       "2     0.444111              4               7   \n",
       "3     0.450000              4               7   \n",
       "4     1.554111              2               5   \n",
       "...        ...            ...             ...   \n",
       "6293  0.187500              4               8   \n",
       "6300  0.000000              3               6   \n",
       "6305  0.250000              3               6   \n",
       "6310  0.160000              3               6   \n",
       "6333  0.139111              4               8   \n",
       "\n",
       "                                           cleaned text  \n",
       "0     Since 2013, CMRB findings have been used to su...  \n",
       "1     UoL’s innovative agri-food research has underp...  \n",
       "2     The Cambridge University research has had a si...  \n",
       "3     Bovine tuberculosis is a substantial economic ...  \n",
       "4     The impact of Bull's work is on: (1) internati...  \n",
       "...                                                 ...  \n",
       "6293  Many scholars, MPs, civil servants and Commons...  \n",
       "6300  Pharm2Farm Ltd. has commercialised NTU's paten...  \n",
       "6305  King’s research has responded practically to t...  \n",
       "6310  UofG’s Institute for Gravitational Research (I...  \n",
       "6333  UltraSoC was spun out to commercialise the deb...  \n",
       "\n",
       "[6286 rows x 23 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('df_gpt4.csv')\n",
    "\n",
    "df = df.dropna(subset=['cleaned text'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 00:46:31.917376: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-28 00:46:31.920218: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-28 00:46:31.968271: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-28 00:46:31.969036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-28 00:46:33.228950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home2/xtdm45/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home2/xtdm45/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home2/xtdm45/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home2/xtdm45/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow\n",
    "from nltk.tokenize import regexp_tokenize, sent_tokenize\n",
    "from nltk import pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "#Dictionary of contractions for replacement \n",
    "\n",
    "contractions = {\n",
    "          \"ain't\": \"am not\",\n",
    "          \"aren't\": \"are not\",\n",
    "          \"can't\": \"cannot\",\n",
    "          \"can't've\": \"cannot have\",\n",
    "          \"'cause\": \"because\",\n",
    "          \"could've\": \"could have\",\n",
    "          \"couldn't\": \"could not\",\n",
    "          \"couldn't've\": \"could not have\",\n",
    "          \"didn't\": \"did not\",\n",
    "          \"doesn't\": \"does not\",\n",
    "          \"don't\": \"do not\",\n",
    "          \"hadn't\": \"had not\",\n",
    "          \"hadn't've\": \"had not have\",\n",
    "          \"hasn't\": \"has not\",\n",
    "          \"haven't\": \"have not\",\n",
    "          \"he'd\": \"he would\",\n",
    "          \"he'd've\": \"he would have\",\n",
    "          \"he'll\": \"he will\",\n",
    "          \"he'll've\": \"he will have\",\n",
    "          \"he's\": \"he is\",\n",
    "          \"how'd\": \"how did\",\n",
    "          \"how'd'y\": \"how do you\",\n",
    "          \"how'll\": \"how will\",\n",
    "          \"how's\": \"how is\",\n",
    "          \"i'd\": \"i would\",\n",
    "          \"i'd've\": \"i would have\",\n",
    "          \"i'll\": \"i will\",\n",
    "          \"i'll've\": \"i will have\",\n",
    "          \"i'm\": \"i am\",\n",
    "          \"i've\": \"i have\",\n",
    "          \"isn't\": \"is not\",\n",
    "          \"it'd\": \"it had\",\n",
    "          \"it'd've\": \"it would have\",\n",
    "          \"it'll\": \"it will\",\n",
    "          \"it'll've\": \"it will have\",\n",
    "          \"it's\": \"it is\",\n",
    "          \"let's\": \"let us\",\n",
    "          \"ma'am\": \"madam\",\n",
    "          \"mayn't\": \"may not\",\n",
    "          \"might've\": \"might have\",\n",
    "          \"mightn't\": \"might not\",\n",
    "          \"mightn't've\": \"might not have\",\n",
    "          \"must've\": \"must have\",\n",
    "          \"mustn't\": \"must not\",\n",
    "          \"mustn't've\": \"must not have\",\n",
    "          \"needn't\": \"need not\",\n",
    "          \"needn't've\": \"need not have\",\n",
    "          \"o'clock\": \"of the clock\",\n",
    "          \"oughtn't\": \"ought not\",\n",
    "          \"oughtn't've\": \"ought not have\",\n",
    "          \"shan't\": \"shall not\",\n",
    "          \"sha'n't\": \"shall not\",\n",
    "          \"shan't've\": \"shall not have\",\n",
    "          \"she'd\": \"she would\",\n",
    "          \"she'd've\": \"she would have\",\n",
    "          \"she'll\": \"she will\",\n",
    "          \"she'll've\": \"she will have\",\n",
    "          \"she's\": \"she is\",\n",
    "          \"should've\": \"should have\",\n",
    "          \"shouldn't\": \"should not\",\n",
    "          \"shouldn't've\": \"should not have\",\n",
    "          \"so've\": \"so have\",\n",
    "          \"so's\": \"so is\",\n",
    "          \"that'd\": \"that would\",\n",
    "          \"that'd've\": \"that would have\",\n",
    "          \"that's\": \"that is\",\n",
    "          \"there'd\": \"there had\",\n",
    "          \"there'd've\": \"there would have\",\n",
    "          \"there's\": \"there is\",\n",
    "          \"they'd\": \"they would\",\n",
    "          \"they'd've\": \"they would have\",\n",
    "          \"they'll\": \"they will\",\n",
    "          \"they'll've\": \"they will have\",\n",
    "          \"they're\": \"they are\",\n",
    "          \"they've\": \"they have\",\n",
    "          \"to've\": \"to have\",\n",
    "          \"wasn't\": \"was not\",\n",
    "          \"we'd\": \"we had\",\n",
    "          \"we'd've\": \"we would have\",\n",
    "          \"we'll\": \"we will\",\n",
    "          \"we'll've\": \"we will have\",\n",
    "          \"we're\": \"we are\",\n",
    "          \"we've\": \"we have\",\n",
    "          \"weren't\": \"were not\",\n",
    "          \"what'll\": \"what will\",\n",
    "          \"what'll've\": \"what will have\",\n",
    "          \"what're\": \"what are\",\n",
    "          \"what's\": \"what is\",\n",
    "          \"what've\": \"what have\",\n",
    "          \"when's\": \"when is\",\n",
    "          \"when've\": \"when have\",\n",
    "          \"where'd\": \"where did\",\n",
    "          \"where's\": \"where is\",\n",
    "          \"where've\": \"where have\",\n",
    "          \"who'll\": \"who will\",\n",
    "          \"who'll've\": \"who will have\",\n",
    "          \"who's\": \"who is\",\n",
    "          \"who've\": \"who have\",\n",
    "          \"why's\": \"why is\",\n",
    "          \"why've\": \"why have\",\n",
    "          \"will've\": \"will have\",\n",
    "          \"won't\": \"will not\",\n",
    "          \"won't've\": \"will not have\",\n",
    "          \"would've\": \"would have\",\n",
    "          \"wouldn't\": \"would not\",\n",
    "          \"wouldn't've\": \"would not have\",\n",
    "          \"y'all\": \"you all\",\n",
    "          \"y'alls\": \"you alls\",\n",
    "          \"y'all'd\": \"you all would\",\n",
    "          \"y'all'd've\": \"you all would have\",\n",
    "          \"y'all're\": \"you all are\",\n",
    "          \"y'all've\": \"you all have\",\n",
    "          \"you'd\": \"you had\",\n",
    "          \"you'd've\": \"you would have\",\n",
    "          \"you'll\": \"you you will\",\n",
    "          \"you'll've\": \"you you will have\",\n",
    "          \"you're\": \"you are\",\n",
    "          \"you've\": \"you have\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "punctuation_set = set(string.punctuation)\n",
    "\n",
    "\n",
    "def penn_to_wordnet(penn_pos_tag):\n",
    "    \"\"\"\n",
    "    Function to convert Penn Treebank part-of-speech tags to corresponding WordNet tags.\n",
    "    \"\"\"\n",
    "    tag_dictionary = {\n",
    "        'NN': 'n', 'NNS': 'n', 'NNP': 'n', 'NNPS': 'n',\n",
    "        'JJ': 'a', 'JJR': 'a', 'JJS': 'a',\n",
    "        'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v',\n",
    "        'RB': 'r', 'RBR': 'r', 'RBS': 'r',\n",
    "        'MD': 'v'\n",
    "    }\n",
    "\n",
    "    # Return the WordNet tag for given Penn Treebank tag, return \"n\" if none found\n",
    "    return tag_dictionary.get(penn_pos_tag[:2], \"n\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess text: lowercasing, removing punctuation, expanding contractions, lemmatization, etc.\n",
    "    \"\"\"\n",
    "    # Account for edge case when text is empty\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert all text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove characters that are repeated more than twice in a row\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # Replace URLs with space\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "\n",
    "    # Remove HTML tags from text\n",
    "    text = re.sub(\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Ensure that there is space after each period\n",
    "    text = re.sub(r'\\.(?![ .])', '. ', text)\n",
    "\n",
    "    # Replace line break tags with space\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "\n",
    "    # Remove words containing digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove all numbers\n",
    "    text = re.sub(r'\\b(?=.*\\d)(?=.*[a-zA-Z]).*?\\b', '', text)\n",
    "\n",
    "    # Remove all numbers including floating points\n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?', '', text)\n",
    "\n",
    "    # Tokenize sentences and words\n",
    "    tokenized_sentences = [regexp_tokenize(sent, pattern=\"\\s+\", gaps=True) for sent in sent_tokenize(text)]\n",
    "\n",
    "    new_sentences = []\n",
    "\n",
    "    # Use tqdm to visualize the progress of processing sentences\n",
    "    for sentences in pos_tag_sents(tokenized_sentences):\n",
    "        new_sentence = []\n",
    "        # Iterate through word/tag pair\n",
    "        for word, tag in sentences:\n",
    "            # Check if word in punctuation and stopwords list\n",
    "            if word not in punctuation_set and word not in stopwords_set:\n",
    "                # Expand contractions if present\n",
    "                if word in contractions:\n",
    "                    words = contractions[word].split(\" \")\n",
    "                    new_sentence.extend(words)\n",
    "                else:\n",
    "                    # Get WordNet Pos Tag and lemmatize word\n",
    "                    wordnet_pos = penn_to_wordnet(tag)\n",
    "                    lemmatized_word = wnl.lemmatize(word, pos=wordnet_pos)\n",
    "                    # Remove non-alphanumeric characters\n",
    "                    lemmatized_word = re.sub(r'[^\\w\\s]', '', lemmatized_word)\n",
    "\n",
    "                    # Check if lemmatized word exists and has length greater than 2\n",
    "                    if lemmatized_word and len(lemmatized_word) > 2:\n",
    "                        new_sentence.append(lemmatized_word)\n",
    "\n",
    "        if new_sentence:\n",
    "            new_sentences.append(new_sentence)\n",
    "\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "new_df = df[[\"Main panel_x\",\"Unit of assessment number_x\",\"Title\",\"cleaned text\",\"5 Class Labels\",\"Summary impact type\",\"9 Class Labels\", \"Variance\", \"Weighted Avg\"]]\n",
    "\n",
    "new_df = new_df[new_df[\"Main panel_x\"] == \"A\"]\n",
    "\n",
    "\n",
    "new_df[\"sentence tokenized\"] = new_df[\"cleaned text\"].apply(lambda x: preprocess_text(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Main panel_x</th>\n",
       "      <th>Unit of assessment number_x</th>\n",
       "      <th>Title</th>\n",
       "      <th>cleaned text</th>\n",
       "      <th>5 Class Labels</th>\n",
       "      <th>Summary impact type</th>\n",
       "      <th>9 Class Labels</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weighted Avg</th>\n",
       "      <th>sentence tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Agricultural Robotics: Data Driven Technologie...</td>\n",
       "      <td>UoL’s innovative agri-food research has underp...</td>\n",
       "      <td>3</td>\n",
       "      <td>Technological</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>[[uols, innovative, agrifood, research, underp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Alleviating the societal and economic burden o...</td>\n",
       "      <td>The Cambridge University research has had a si...</td>\n",
       "      <td>4</td>\n",
       "      <td>Health</td>\n",
       "      <td>7</td>\n",
       "      <td>0.444111</td>\n",
       "      <td>3.667</td>\n",
       "      <td>[[cambridge, university, research, significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Changes in policy for the control of bovine tu...</td>\n",
       "      <td>Bovine tuberculosis is a substantial economic ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>7</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>3.500</td>\n",
       "      <td>[[bovine, tuberculosis, substantial, economic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Empowering personalised discussion and advance...</td>\n",
       "      <td>Advance decisions about resuscitation are a gl...</td>\n",
       "      <td>4</td>\n",
       "      <td>Health</td>\n",
       "      <td>7</td>\n",
       "      <td>0.358124</td>\n",
       "      <td>3.626</td>\n",
       "      <td>[[advance, decision, resuscitation, global, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Improving patient outcomes and treatment guide...</td>\n",
       "      <td>Professor Irving's research has been instrumen...</td>\n",
       "      <td>3</td>\n",
       "      <td>Health</td>\n",
       "      <td>6</td>\n",
       "      <td>0.454204</td>\n",
       "      <td>3.214</td>\n",
       "      <td>[[professor, irvings, research, instrumental, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6262</th>\n",
       "      <td>A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Transforming the use of Arterial Spin Labellin...</td>\n",
       "      <td>The Arterial Spin Labelling In Dementia (AID) ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Technological</td>\n",
       "      <td>7</td>\n",
       "      <td>0.655264</td>\n",
       "      <td>3.644</td>\n",
       "      <td>[[arterial, spin, labelling, dementia, aid, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6271</th>\n",
       "      <td>A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Transforming therapeutic outcomes of fatal Duc...</td>\n",
       "      <td>Research undertaken at RHUL led to the develop...</td>\n",
       "      <td>3</td>\n",
       "      <td>Health</td>\n",
       "      <td>7</td>\n",
       "      <td>0.222111</td>\n",
       "      <td>3.333</td>\n",
       "      <td>[[research, undertaken, rhul, lead, developmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6275</th>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Transforming Treatment for Obstructive Sleep A...</td>\n",
       "      <td>Obstructive sleep apnoea (OSA) is one of the m...</td>\n",
       "      <td>3</td>\n",
       "      <td>Health</td>\n",
       "      <td>7</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>3.375</td>\n",
       "      <td>[[obstructive, sleep, apnoea, osa, one, common...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6289</th>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Transforming vaccine policy for pneumococcal d...</td>\n",
       "      <td>It was the result of Professor Lim's expertise...</td>\n",
       "      <td>3</td>\n",
       "      <td>Health</td>\n",
       "      <td>7</td>\n",
       "      <td>0.334111</td>\n",
       "      <td>3.333</td>\n",
       "      <td>[[result, professor, lims, expertise, field, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Translating biomedical nanotechnology into com...</td>\n",
       "      <td>Pharm2Farm Ltd. has commercialised NTU's paten...</td>\n",
       "      <td>3</td>\n",
       "      <td>Technological</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>[[pharmfarm, ltd, commercialise, ntus, patent,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1350 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Main panel_x  Unit of assessment number_x  \\\n",
       "1               A                          6.0   \n",
       "2               A                          4.0   \n",
       "3               A                          6.0   \n",
       "7               A                          2.0   \n",
       "17              A                          5.0   \n",
       "...           ...                          ...   \n",
       "6262            A                          4.0   \n",
       "6271            A                          5.0   \n",
       "6275            A                          3.0   \n",
       "6289            A                          1.0   \n",
       "6300            A                          3.0   \n",
       "\n",
       "                                                  Title  \\\n",
       "1     Agricultural Robotics: Data Driven Technologie...   \n",
       "2     Alleviating the societal and economic burden o...   \n",
       "3     Changes in policy for the control of bovine tu...   \n",
       "7     Empowering personalised discussion and advance...   \n",
       "17    Improving patient outcomes and treatment guide...   \n",
       "...                                                 ...   \n",
       "6262  Transforming the use of Arterial Spin Labellin...   \n",
       "6271  Transforming therapeutic outcomes of fatal Duc...   \n",
       "6275  Transforming Treatment for Obstructive Sleep A...   \n",
       "6289  Transforming vaccine policy for pneumococcal d...   \n",
       "6300  Translating biomedical nanotechnology into com...   \n",
       "\n",
       "                                           cleaned text  5 Class Labels  \\\n",
       "1     UoL’s innovative agri-food research has underp...               3   \n",
       "2     The Cambridge University research has had a si...               4   \n",
       "3     Bovine tuberculosis is a substantial economic ...               4   \n",
       "7     Advance decisions about resuscitation are a gl...               4   \n",
       "17    Professor Irving's research has been instrumen...               3   \n",
       "...                                                 ...             ...   \n",
       "6262  The Arterial Spin Labelling In Dementia (AID) ...               4   \n",
       "6271  Research undertaken at RHUL led to the develop...               3   \n",
       "6275  Obstructive sleep apnoea (OSA) is one of the m...               3   \n",
       "6289  It was the result of Professor Lim's expertise...               3   \n",
       "6300  Pharm2Farm Ltd. has commercialised NTU's paten...               3   \n",
       "\n",
       "     Summary impact type  9 Class Labels  Variance  Weighted Avg  \\\n",
       "1          Technological               6  0.666000         3.000   \n",
       "2                 Health               7  0.444111         3.667   \n",
       "3          Environmental               7  0.450000         3.500   \n",
       "7                 Health               7  0.358124         3.626   \n",
       "17                Health               6  0.454204         3.214   \n",
       "...                  ...             ...       ...           ...   \n",
       "6262       Technological               7  0.655264         3.644   \n",
       "6271              Health               7  0.222111         3.333   \n",
       "6275              Health               7  0.234375         3.375   \n",
       "6289              Health               7  0.334111         3.333   \n",
       "6300       Technological               6  0.000000         3.000   \n",
       "\n",
       "                                     sentence tokenized  \n",
       "1     [[uols, innovative, agrifood, research, underp...  \n",
       "2     [[cambridge, university, research, significant...  \n",
       "3     [[bovine, tuberculosis, substantial, economic,...  \n",
       "7     [[advance, decision, resuscitation, global, he...  \n",
       "17    [[professor, irvings, research, instrumental, ...  \n",
       "...                                                 ...  \n",
       "6262  [[arterial, spin, labelling, dementia, aid, pr...  \n",
       "6271  [[research, undertaken, rhul, lead, developmen...  \n",
       "6275  [[obstructive, sleep, apnoea, osa, one, common...  \n",
       "6289  [[result, professor, lims, expertise, field, l...  \n",
       "6300  [[pharmfarm, ltd, commercialise, ntus, patent,...  \n",
       "\n",
       "[1350 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(sents_tokenized):\n",
    "    return [word for sentence in sents_tokenized for word in sentence]\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "    \n",
    "new_df[\"word tokenized\"] = new_df[\"sentence tokenized\"].apply(lambda x: flatten(x))\n",
    "\n",
    "new_df[\"string\"] = new_df[\"word tokenized\"].apply(lambda x: tokens_to_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(label):\n",
    "    if label in [0, 1, 2, 3, 4, 5]:\n",
    "        return 0\n",
    "    elif label in [6, 7]:\n",
    "        return 1\n",
    "    elif label == 8:\n",
    "        return 2\n",
    "    else:\n",
    "        return label\n",
    "    \n",
    "new_df['3 Class Labels'] = new_df['9 Class Labels'].apply(map_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batched_text):\n",
    "    return tokenizer(batched_text['text'], padding = 'max_length', truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>The research of Wormstone and team in developi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>Driving conservation efforts through establish...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Bariatric surgery is a widely used interventio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>Perinatal depression can have devastating cons...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5719</th>\n",
       "      <td>Professor Stallard's research has improved the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>Estimates of the burden of viral hepatitis wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>The landscape of the emergency care workforce ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6027</th>\n",
       "      <td>Details of the impact. New diagnostic testing ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>Details of the impact\\n\\nHaving identified a c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>According to the World Health Organisation, mu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>945 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "93    The research of Wormstone and team in developi...      1\n",
       "2617  Driving conservation efforts through establish...      1\n",
       "874   Bariatric surgery is a widely used interventio...      0\n",
       "2747  Perinatal depression can have devastating cons...      2\n",
       "5719  Professor Stallard's research has improved the...      1\n",
       "...                                                 ...    ...\n",
       "4785  Estimates of the burden of viral hepatitis wer...      1\n",
       "5026  The landscape of the emergency care workforce ...      1\n",
       "6027  Details of the impact. New diagnostic testing ...      1\n",
       "3891  Details of the impact\\n\\nHaving identified a c...      1\n",
       "5002  According to the World Health Organisation, mu...      2\n",
       "\n",
       "[945 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = result_df[\"3 Class Labels\"]\n",
    "texts = result_df[\"cleaned text\"]\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n",
    "val_df = pd.DataFrame({'text': val_texts, 'label': val_labels})\n",
    "test_df = pd.DataFrame({'text': test_texts, 'label': test_labels})\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [02:24<00:00,  1.38it/s]\n",
      "100%|██████████| 250/250 [03:00<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "\n",
    "def augment_text(df, samples, class_id):\n",
    "    contextual_aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action='substitute', device='cuda')\n",
    "\n",
    "    new_texts = []\n",
    "    \n",
    "    df_minority = df[df[\"label\"] == class_id].reset_index(drop=True)\n",
    "    \n",
    "    for i in tqdm(np.random.randint(0, len(df_minority), samples)):\n",
    "        text = df_minority.iloc[i]['text']\n",
    "        augmented_text = contextual_aug.augment(text)\n",
    "        new_texts.append(augmented_text[0])\n",
    "    \n",
    "    new_df = pd.DataFrame({'text': new_texts, 'label': class_id})\n",
    "\n",
    "    df = shuffle(pd.concat([df, new_df]).reset_index(drop=True))\n",
    "    \n",
    "    return df\n",
    "\n",
    "augmented_df = augment_text(train_df, 200, 0)\n",
    "\n",
    "augmented_df = augment_text(augmented_df, 250, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    615\n",
       "2    467\n",
       "0    313\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__'],\n",
      "        num_rows: 1395\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__'],\n",
      "        num_rows: 203\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(augmented_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "val_dataset =  Dataset.from_pandas(val_df)\n",
    "\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1395/1395 [00:02<00:00, 526.46 examples/s]\n",
      "Map: 100%|██████████| 203/203 [00:00<00:00, 522.00 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 528.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "text_encoded = dataset_dict.map(tokenization, batched=True, batch_size=None)\n",
    "\n",
    "text_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size 4, learning rate 2e-05, num_epochs 2, weight_decay 0.0, warmup_steps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='174' max='174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [174/174 02:33, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.948400</td>\n",
       "      <td>0.870236</td>\n",
       "      <td>0.569307</td>\n",
       "      <td>0.551040</td>\n",
       "      <td>0.579036</td>\n",
       "      <td>0.569307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.837933</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.600558</td>\n",
       "      <td>0.613692</td>\n",
       "      <td>0.599010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.8379329442977905, 'eval_accuracy': 0.599009900990099, 'eval_f1': 0.6005578484472754, 'eval_precision': 0.6136918097045466, 'eval_recall': 0.599009900990099, 'eval_runtime': 2.8532, 'eval_samples_per_second': 70.797, 'eval_steps_per_second': 9.113, 'epoch': 1.99}\n",
      "Current best params: {'per_gpu_batch_size': 4, 'learning_rate': 2e-05, 'num_epochs': 2, 'weight_decay': 0.0, 'warmup_steps': 0} with F1 score: 0.6005578484472754\n",
      "Training with batch size 4, learning rate 2e-05, num_epochs 2, weight_decay 0.0, warmup_steps 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/home2/xtdm45/mds-project/topic_modelling_2/lib/python3.8/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 47/174 00:32 < 01:31, 1.38 it/s, Epoch 0.53/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1064072/3232716494.py\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mds-project/topic_modelling_2/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1538\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mds-project/topic_modelling_2/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m                 if (\n",
      "\u001b[0;32m~/mds-project/topic_modelling_2/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mds-project/topic_modelling_2/lib/python3.8/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mds-project/topic_modelling_2/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
      "\u001b[0;32m~/mds-project/topic_modelling_2/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mds-project/topic_modelling_2/lib/python3.8/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n",
    "from itertools import product\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "\n",
    "   \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "   \n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "param_grid = {\n",
    "    \"per_gpu_batch_size\": [4, 8],\n",
    "    \"learning_rate\": [2e-5, 3e-5, 5e-5],\n",
    "    \"num_epochs\": [2, 3, 4],\n",
    "    \"weight_decay\": [0.0, 0.01, 0.1, 0.0001, 0.001],  # Added weight decay values\n",
    "    \"warmup_steps\": [0, 100, 200, 500, 1000]  # Added warmup steps values\n",
    "}\n",
    "\n",
    "# Get all combinations of hyperparameters\n",
    "param_combinations = list(product(\n",
    "    param_grid['per_gpu_batch_size'],\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['num_epochs'],\n",
    "    param_grid['weight_decay'],\n",
    "    param_grid['warmup_steps']\n",
    "))\n",
    "\n",
    "results = []\n",
    "\n",
    "best_eval_result = None\n",
    "best_params = None\n",
    "best_f1 = 0.0  \n",
    "\n",
    "for per_gpu_batch_size, learning_rate, num_epochs, weight_decay, warmup_steps in param_combinations:\n",
    "    print(f\"Training with batch size {per_gpu_batch_size}, learning rate {learning_rate}, num_epochs {num_epochs}, weight_decay {weight_decay}, warmup_steps {warmup_steps}\")\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "    model.to(device)    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          \n",
    "        num_train_epochs=num_epochs,              \n",
    "        per_device_train_batch_size=per_gpu_batch_size,   \n",
    "        evaluation_strategy=\"epoch\",     \n",
    "        save_strategy=\"epoch\",  \n",
    "        fp16=True,  \n",
    "        gradient_accumulation_steps=4, \n",
    "        save_total_limit=1,              \n",
    "        load_best_model_at_end=True,     \n",
    "        learning_rate=learning_rate,              \n",
    "        weight_decay=weight_decay,              \n",
    "        logging_dir='./logs',            \n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",                \n",
    "        warmup_steps=warmup_steps  \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=text_encoded['train'],\n",
    "        eval_dataset=text_encoded['validation'],\n",
    "        compute_metrics=compute_metrics  # This function should return a dictionary with 'f1' key for F1 score\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_result = trainer.evaluate()\n",
    "\n",
    "    print(f\"Evaluation results: {eval_result}\")\n",
    "\n",
    "    results.append({\n",
    "        \"per_gpu_batch_size\": per_gpu_batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"f1\": eval_result['eval_f1'],  \n",
    "        \"acc\": eval_result['eval_accuracy']  \n",
    "    })\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if eval_result['eval_f1'] > best_f1:\n",
    "        best_f1 = eval_result['eval_f1']\n",
    "        best_eval_result = eval_result\n",
    "        best_params = {\n",
    "            \"per_gpu_batch_size\": per_gpu_batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"warmup_steps\": warmup_steps\n",
    "        }\n",
    "        model.save_pretrained('./best_model')\n",
    "        tokenizer.save_pretrained('./best_tokenizer')\n",
    "\n",
    "    print(f\"Current best params: {best_params} with F1 score: {best_f1}\")\n",
    "\n",
    "print(f\"Best hyperparameters found: {best_params}\")\n",
    "print(f\"Best evaluation results: {best_eval_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>per_gpu_batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>f1</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>2</td>\n",
       "      <td>0.596149</td>\n",
       "      <td>0.594059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>3</td>\n",
       "      <td>0.506367</td>\n",
       "      <td>0.504950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>4</td>\n",
       "      <td>0.618383</td>\n",
       "      <td>0.638614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.577290</td>\n",
       "      <td>0.584158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>3</td>\n",
       "      <td>0.585873</td>\n",
       "      <td>0.584158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>4</td>\n",
       "      <td>0.457015</td>\n",
       "      <td>0.470297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>2</td>\n",
       "      <td>0.529366</td>\n",
       "      <td>0.529703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3</td>\n",
       "      <td>0.585575</td>\n",
       "      <td>0.599010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4</td>\n",
       "      <td>0.564446</td>\n",
       "      <td>0.574257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>2</td>\n",
       "      <td>0.572737</td>\n",
       "      <td>0.569307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>3</td>\n",
       "      <td>0.554317</td>\n",
       "      <td>0.549505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>4</td>\n",
       "      <td>0.588892</td>\n",
       "      <td>0.608911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.543628</td>\n",
       "      <td>0.539604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>3</td>\n",
       "      <td>0.621791</td>\n",
       "      <td>0.618812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>4</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.584158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>2</td>\n",
       "      <td>0.585178</td>\n",
       "      <td>0.584158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3</td>\n",
       "      <td>0.581023</td>\n",
       "      <td>0.579208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4</td>\n",
       "      <td>0.580709</td>\n",
       "      <td>0.579208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    per_gpu_batch_size  learning_rate  num_epochs        f1       acc\n",
       "0                    4        0.00002           2  0.596149  0.594059\n",
       "1                    4        0.00002           3  0.506367  0.504950\n",
       "2                    4        0.00002           4  0.618383  0.638614\n",
       "3                    4        0.00003           2  0.577290  0.584158\n",
       "4                    4        0.00003           3  0.585873  0.584158\n",
       "5                    4        0.00003           4  0.457015  0.470297\n",
       "6                    4        0.00005           2  0.529366  0.529703\n",
       "7                    4        0.00005           3  0.585575  0.599010\n",
       "8                    4        0.00005           4  0.564446  0.574257\n",
       "9                    8        0.00002           2  0.572737  0.569307\n",
       "10                   8        0.00002           3  0.554317  0.549505\n",
       "11                   8        0.00002           4  0.588892  0.608911\n",
       "12                   8        0.00003           2  0.543628  0.539604\n",
       "13                   8        0.00003           3  0.621791  0.618812\n",
       "14                   8        0.00003           4  0.587700  0.584158\n",
       "15                   8        0.00005           2  0.585178  0.584158\n",
       "16                   8        0.00005           3  0.581023  0.579208\n",
       "17                   8        0.00005           4  0.580709  0.579208"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results\n",
    "\n",
    "df_results.to_csv('hyperparameter_results.csv', index=False)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    per_gpu_batch_size  learning_rate  num_epochs        f1       acc\n",
      "0                    4        0.00002           2  0.596149  0.594059\n",
      "1                    4        0.00002           3  0.506367  0.504950\n",
      "2                    4        0.00002           4  0.618383  0.638614\n",
      "3                    4        0.00003           2  0.577290  0.584158\n",
      "4                    4        0.00003           3  0.585873  0.584158\n",
      "5                    4        0.00003           4  0.457015  0.470297\n",
      "6                    4        0.00005           2  0.529366  0.529703\n",
      "7                    4        0.00005           3  0.585575  0.599010\n",
      "8                    4        0.00005           4  0.564446  0.574257\n",
      "9                    8        0.00002           2  0.572737  0.569307\n",
      "10                   8        0.00002           3  0.554317  0.549505\n",
      "11                   8        0.00002           4  0.588892  0.608911\n",
      "12                   8        0.00003           2  0.543628  0.539604\n",
      "13                   8        0.00003           3  0.621791  0.618812\n",
      "14                   8        0.00003           4  0.587700  0.584158\n",
      "15                   8        0.00005           2  0.585178  0.584158\n",
      "16                   8        0.00005           3  0.581023  0.579208\n",
      "17                   8        0.00005           4  0.580709  0.579208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.read_csv('hyperparameter_results.csv')\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Topic Modelling 2",
   "language": "python",
   "name": "topic_modelling_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
