{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 21:54:17.458939: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-06 21:54:17.461751: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-06 21:54:17.509057: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-06 21:54:17.510427: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-06 21:54:18.843769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home2/xtdm45/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home2/xtdm45/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home2/xtdm45/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home2/xtdm45/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow\n",
    "from nltk.tokenize import regexp_tokenize, sent_tokenize\n",
    "from nltk import pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "#Dictionary of contractions for replacement \n",
    "\n",
    "contractions = {\n",
    "          \"ain't\": \"am not\",\n",
    "          \"aren't\": \"are not\",\n",
    "          \"can't\": \"cannot\",\n",
    "          \"can't've\": \"cannot have\",\n",
    "          \"'cause\": \"because\",\n",
    "          \"could've\": \"could have\",\n",
    "          \"couldn't\": \"could not\",\n",
    "          \"couldn't've\": \"could not have\",\n",
    "          \"didn't\": \"did not\",\n",
    "          \"doesn't\": \"does not\",\n",
    "          \"don't\": \"do not\",\n",
    "          \"hadn't\": \"had not\",\n",
    "          \"hadn't've\": \"had not have\",\n",
    "          \"hasn't\": \"has not\",\n",
    "          \"haven't\": \"have not\",\n",
    "          \"he'd\": \"he would\",\n",
    "          \"he'd've\": \"he would have\",\n",
    "          \"he'll\": \"he will\",\n",
    "          \"he'll've\": \"he will have\",\n",
    "          \"he's\": \"he is\",\n",
    "          \"how'd\": \"how did\",\n",
    "          \"how'd'y\": \"how do you\",\n",
    "          \"how'll\": \"how will\",\n",
    "          \"how's\": \"how is\",\n",
    "          \"i'd\": \"i would\",\n",
    "          \"i'd've\": \"i would have\",\n",
    "          \"i'll\": \"i will\",\n",
    "          \"i'll've\": \"i will have\",\n",
    "          \"i'm\": \"i am\",\n",
    "          \"i've\": \"i have\",\n",
    "          \"isn't\": \"is not\",\n",
    "          \"it'd\": \"it had\",\n",
    "          \"it'd've\": \"it would have\",\n",
    "          \"it'll\": \"it will\",\n",
    "          \"it'll've\": \"it will have\",\n",
    "          \"it's\": \"it is\",\n",
    "          \"let's\": \"let us\",\n",
    "          \"ma'am\": \"madam\",\n",
    "          \"mayn't\": \"may not\",\n",
    "          \"might've\": \"might have\",\n",
    "          \"mightn't\": \"might not\",\n",
    "          \"mightn't've\": \"might not have\",\n",
    "          \"must've\": \"must have\",\n",
    "          \"mustn't\": \"must not\",\n",
    "          \"mustn't've\": \"must not have\",\n",
    "          \"needn't\": \"need not\",\n",
    "          \"needn't've\": \"need not have\",\n",
    "          \"o'clock\": \"of the clock\",\n",
    "          \"oughtn't\": \"ought not\",\n",
    "          \"oughtn't've\": \"ought not have\",\n",
    "          \"shan't\": \"shall not\",\n",
    "          \"sha'n't\": \"shall not\",\n",
    "          \"shan't've\": \"shall not have\",\n",
    "          \"she'd\": \"she would\",\n",
    "          \"she'd've\": \"she would have\",\n",
    "          \"she'll\": \"she will\",\n",
    "          \"she'll've\": \"she will have\",\n",
    "          \"she's\": \"she is\",\n",
    "          \"should've\": \"should have\",\n",
    "          \"shouldn't\": \"should not\",\n",
    "          \"shouldn't've\": \"should not have\",\n",
    "          \"so've\": \"so have\",\n",
    "          \"so's\": \"so is\",\n",
    "          \"that'd\": \"that would\",\n",
    "          \"that'd've\": \"that would have\",\n",
    "          \"that's\": \"that is\",\n",
    "          \"there'd\": \"there had\",\n",
    "          \"there'd've\": \"there would have\",\n",
    "          \"there's\": \"there is\",\n",
    "          \"they'd\": \"they would\",\n",
    "          \"they'd've\": \"they would have\",\n",
    "          \"they'll\": \"they will\",\n",
    "          \"they'll've\": \"they will have\",\n",
    "          \"they're\": \"they are\",\n",
    "          \"they've\": \"they have\",\n",
    "          \"to've\": \"to have\",\n",
    "          \"wasn't\": \"was not\",\n",
    "          \"we'd\": \"we had\",\n",
    "          \"we'd've\": \"we would have\",\n",
    "          \"we'll\": \"we will\",\n",
    "          \"we'll've\": \"we will have\",\n",
    "          \"we're\": \"we are\",\n",
    "          \"we've\": \"we have\",\n",
    "          \"weren't\": \"were not\",\n",
    "          \"what'll\": \"what will\",\n",
    "          \"what'll've\": \"what will have\",\n",
    "          \"what're\": \"what are\",\n",
    "          \"what's\": \"what is\",\n",
    "          \"what've\": \"what have\",\n",
    "          \"when's\": \"when is\",\n",
    "          \"when've\": \"when have\",\n",
    "          \"where'd\": \"where did\",\n",
    "          \"where's\": \"where is\",\n",
    "          \"where've\": \"where have\",\n",
    "          \"who'll\": \"who will\",\n",
    "          \"who'll've\": \"who will have\",\n",
    "          \"who's\": \"who is\",\n",
    "          \"who've\": \"who have\",\n",
    "          \"why's\": \"why is\",\n",
    "          \"why've\": \"why have\",\n",
    "          \"will've\": \"will have\",\n",
    "          \"won't\": \"will not\",\n",
    "          \"won't've\": \"will not have\",\n",
    "          \"would've\": \"would have\",\n",
    "          \"wouldn't\": \"would not\",\n",
    "          \"wouldn't've\": \"would not have\",\n",
    "          \"y'all\": \"you all\",\n",
    "          \"y'alls\": \"you alls\",\n",
    "          \"y'all'd\": \"you all would\",\n",
    "          \"y'all'd've\": \"you all would have\",\n",
    "          \"y'all're\": \"you all are\",\n",
    "          \"y'all've\": \"you all have\",\n",
    "          \"you'd\": \"you had\",\n",
    "          \"you'd've\": \"you would have\",\n",
    "          \"you'll\": \"you you will\",\n",
    "          \"you'll've\": \"you you will have\",\n",
    "          \"you're\": \"you are\",\n",
    "          \"you've\": \"you have\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1454231/3844787731.py:25: DtypeWarning: Columns (0,3,22,24,34,35,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ref_outputs = pd.read_csv(\"REF 2021 Outputs - All - 2023-09-08.csv\", encoding=\"latin1\", skiprows = 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Institution UKPRN code</th>\n",
       "      <th>Institution name</th>\n",
       "      <th>Main panel</th>\n",
       "      <th>Unit of assessment number</th>\n",
       "      <th>Unit of assessment name</th>\n",
       "      <th>Multiple submission letter</th>\n",
       "      <th>Multiple submission name</th>\n",
       "      <th>Joint submission</th>\n",
       "      <th>Output type</th>\n",
       "      <th>Title</th>\n",
       "      <th>Place</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Volume title</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>First page</th>\n",
       "      <th>Article number</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Patent number</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>URL</th>\n",
       "      <th>Number of additional authors</th>\n",
       "      <th>Non-English</th>\n",
       "      <th>Interdisciplinary</th>\n",
       "      <th>Forensic science</th>\n",
       "      <th>Criminology</th>\n",
       "      <th>Propose double weighting</th>\n",
       "      <th>Is reserve output</th>\n",
       "      <th>Research group</th>\n",
       "      <th>Open access status</th>\n",
       "      <th>Citations applicable</th>\n",
       "      <th>Citation count</th>\n",
       "      <th>Cross-referral requested</th>\n",
       "      <th>Supplementary information</th>\n",
       "      <th>Delayed by COVID19</th>\n",
       "      <th>REF2ID</th>\n",
       "      <th>Incl sig material before 2014</th>\n",
       "      <th>Incl reseach process</th>\n",
       "      <th>Incl factual info about significance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10007760</td>\n",
       "      <td>Birkbeck College</td>\n",
       "      <td>D</td>\n",
       "      <td>26</td>\n",
       "      <td>Modern Languages and Linguistics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Text on Image on Text: Picturing âNatureâ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>University of Delaware Press</td>\n",
       "      <td>Visualizing the Text From Manuscript Culture  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>978-1611496451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Out of scope for open access requirements</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9062d547-00c8-47a2-bd0c-4ea4956ffb85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10007848</td>\n",
       "      <td>University of Chester</td>\n",
       "      <td>C</td>\n",
       "      <td>14</td>\n",
       "      <td>Geography and Environmental Studies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>\"Digital by Default\" and \"the hard to reach\": ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local Economy</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0269-0942</td>\n",
       "      <td>10.1177/0269094216670938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6ad5c192-a622-4468-be71-dba2570735bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10007855</td>\n",
       "      <td>Swansea University / Prifysgol Abertawe</td>\n",
       "      <td>D</td>\n",
       "      <td>27</td>\n",
       "      <td>English Language and Literature</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>\"Stand not on that brink!\": Byron, gender and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Edinburgh University Press</td>\n",
       "      <td>Byron and the Margins of Romanticism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>978 1 4744 3941 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>December</td>\n",
       "      <td>2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Out of scope for open access requirements</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62498e62-48e0-4ccc-8aec-ccbfb72ace06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10007788</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>D</td>\n",
       "      <td>33</td>\n",
       "      <td>Music, Drama, Dance, Performing Arts, Film and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>' . . . nur ein Gleichnis': Heinrich Schenker ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Music and Letters</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1477-4631</td>\n",
       "      <td>10.1093/ml/gcz021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>August</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Compliant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ff3324db-912d-4a3b-97fd-547881617986</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004775</td>\n",
       "      <td>Norwich University of the Arts</td>\n",
       "      <td>D</td>\n",
       "      <td>32</td>\n",
       "      <td>Art and Design: History, Practice and Theory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>âBuilding with Light. Spatial Qualities of t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mendrisio Academy Press / Silvana Editoriale C...</td>\n",
       "      <td>Manipulating light in premodern times / Manipo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9788836627219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.researchgate.net/profile/Vladimir-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A - Created and Contested Territories</td>\n",
       "      <td>Out of scope for open access requirements</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>454d1cdb-06f9-47e8-bf91-cbaf9029ef73</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 Institution UKPRN code                         Institution name Main panel  \\\n",
       "0               10007760                         Birkbeck College          D   \n",
       "1               10007848                    University of Chester          C   \n",
       "2               10007855  Swansea University / Prifysgol Abertawe          D   \n",
       "3               10007788                  University of Cambridge          D   \n",
       "4               10004775           Norwich University of the Arts          D   \n",
       "\n",
       "0 Unit of assessment number  \\\n",
       "0                        26   \n",
       "1                        14   \n",
       "2                        27   \n",
       "3                        33   \n",
       "4                        32   \n",
       "\n",
       "0                            Unit of assessment name  \\\n",
       "0                   Modern Languages and Linguistics   \n",
       "1                Geography and Environmental Studies   \n",
       "2                    English Language and Literature   \n",
       "3  Music, Drama, Dance, Performing Arts, Film and...   \n",
       "4       Art and Design: History, Practice and Theory   \n",
       "\n",
       "0 Multiple submission letter Multiple submission name Joint submission  \\\n",
       "0                        NaN                      NaN              NaN   \n",
       "1                        NaN                      NaN              NaN   \n",
       "2                        NaN                      NaN              NaN   \n",
       "3                        NaN                      NaN              NaN   \n",
       "4                        NaN                      NaN              NaN   \n",
       "\n",
       "0 Output type                                              Title Place  \\\n",
       "0           C  Text on Image on Text: Picturing âNatureâ ...   NaN   \n",
       "1           D  \"Digital by Default\" and \"the hard to reach\": ...   NaN   \n",
       "2           C  \"Stand not on that brink!\": Byron, gender and ...   NaN   \n",
       "3           D  ' . . . nur ein Gleichnis': Heinrich Schenker ...   NaN   \n",
       "4           C  âBuilding with Light. Spatial Qualities of t...   NaN   \n",
       "\n",
       "0                                          Publisher  \\\n",
       "0                       University of Delaware Press   \n",
       "1                                                NaN   \n",
       "2                         Edinburgh University Press   \n",
       "3                                                NaN   \n",
       "4  Mendrisio Academy Press / Silvana Editoriale C...   \n",
       "\n",
       "0                                       Volume title Volume Issue First page  \\\n",
       "0  Visualizing the Text From Manuscript Culture  ...    NaN   NaN        NaN   \n",
       "1                                      Local Economy     31     7        757   \n",
       "2               Byron and the Margins of Romanticism    NaN   NaN        207   \n",
       "3                                  Music and Letters    100     2        271   \n",
       "4  Manipulating light in premodern times / Manipo...    NaN   NaN        NaN   \n",
       "\n",
       "0 Article number               ISBN       ISSN                       DOI  \\\n",
       "0            NaN     978-1611496451        NaN                       NaN   \n",
       "1            NaN                NaN  0269-0942  10.1177/0269094216670938   \n",
       "2            NaN  978 1 4744 3941 1        NaN                       NaN   \n",
       "3            NaN                NaN  1477-4631         10.1093/ml/gcz021   \n",
       "4            NaN      9788836627219        NaN                       NaN   \n",
       "\n",
       "0 Patent number      Month  Year  \\\n",
       "0           NaN        NaN  2017   \n",
       "1           NaN  September  2016   \n",
       "2           NaN   December  2018   \n",
       "3           NaN     August  2019   \n",
       "4           NaN        NaN  2014   \n",
       "\n",
       "0                                                URL  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  https://www.researchgate.net/profile/Vladimir-...   \n",
       "\n",
       "0 Number of additional authors Non-English Interdisciplinary Forensic science  \\\n",
       "0                          NaN         NaN               NaN              NaN   \n",
       "1                            3         NaN               NaN              NaN   \n",
       "2                            0         NaN               NaN              NaN   \n",
       "3                            0         NaN               NaN              NaN   \n",
       "4                          NaN         NaN               NaN              NaN   \n",
       "\n",
       "0 Criminology Propose double weighting Is reserve output  \\\n",
       "0         NaN                      NaN               Yes   \n",
       "1         NaN                      NaN               NaN   \n",
       "2         NaN                      NaN               NaN   \n",
       "3         NaN                      NaN               NaN   \n",
       "4         NaN                      NaN               NaN   \n",
       "\n",
       "0                         Research group  \\\n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4  A - Created and Contested Territories   \n",
       "\n",
       "0                         Open access status Citations applicable  \\\n",
       "0  Out of scope for open access requirements                  NaN   \n",
       "1                                  Compliant                  NaN   \n",
       "2  Out of scope for open access requirements                  NaN   \n",
       "3                                  Compliant                  NaN   \n",
       "4  Out of scope for open access requirements                  NaN   \n",
       "\n",
       "0 Citation count Cross-referral requested Supplementary information  \\\n",
       "0            NaN                      NaN                       NaN   \n",
       "1            NaN                      NaN                       NaN   \n",
       "2            NaN                      NaN                       NaN   \n",
       "3            NaN                      NaN                       NaN   \n",
       "4            NaN                      NaN                       NaN   \n",
       "\n",
       "0 Delayed by COVID19                                REF2ID  \\\n",
       "0                NaN  9062d547-00c8-47a2-bd0c-4ea4956ffb85   \n",
       "1                NaN  6ad5c192-a622-4468-be71-dba2570735bf   \n",
       "2                NaN  62498e62-48e0-4ccc-8aec-ccbfb72ace06   \n",
       "3                NaN  ff3324db-912d-4a3b-97fd-547881617986   \n",
       "4                NaN  454d1cdb-06f9-47e8-bf91-cbaf9029ef73   \n",
       "\n",
       "0 Incl sig material before 2014 Incl reseach process  \\\n",
       "0                             0                    0   \n",
       "1                             0                    0   \n",
       "2                             0                    0   \n",
       "3                             0                    0   \n",
       "4                             0                    1   \n",
       "\n",
       "0 Incl factual info about significance  \n",
       "0                                    0  \n",
       "1                                    0  \n",
       "2                                    0  \n",
       "3                                    0  \n",
       "4                                    0  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ics = pd.read_csv(\"REF 2021 Impact Case Studies - Extract - 2024-05-16.csv\", encoding=\"latin1\")\n",
    "\n",
    "ics_results = pd.read_csv('REF 2021 Results - All - 2022-05-06.csv', encoding=\"latin1\", skiprows=5)\n",
    "\n",
    "ics_results.columns = ics_results.iloc[0]\n",
    "\n",
    "ics_results = ics_results[1:]\n",
    "\n",
    "ics_results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ics_results.columns = ics_results.columns.astype(str)\n",
    "\n",
    "ecs = pd.read_csv(\"REF 2021 Environment Data - All - 2022-06-21.csv\", skiprows = 3)\n",
    "\n",
    "ecs.columns = ecs.iloc[0]\n",
    "\n",
    "ecs = ecs[1:]\n",
    "\n",
    "ecs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ref_outputs = pd.read_csv(\"REF 2021 Outputs - All - 2023-09-08.csv\", encoding=\"latin1\", skiprows = 3)\n",
    "\n",
    "ref_outputs.columns = ref_outputs.iloc[0]\n",
    "\n",
    "ref_outputs = ref_outputs[1:]\n",
    "\n",
    "ref_outputs.reset_index(drop = True, inplace = True)\n",
    "\n",
    "ref_outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_weighted_average(per_of_4_stars, per_of_3_stars, per_of_2_stars, per_of_1_stars, per_of_undefined):\n",
    "    \n",
    "    \n",
    "    star_counts = np.array([per_of_4_stars, per_of_3_stars, per_of_2_stars, per_of_1_stars, per_of_undefined])/ 100\n",
    "    \n",
    "    star_values = np.array([4, 3, 2, 1, 0])\n",
    "    \n",
    "    return np.dot(star_counts, star_values)  \n",
    "\n",
    "\n",
    "def calculate_variance(per_of_4_stars, per_of_3_stars, per_of_2_stars, per_of_1_stars, per_of_undefined, mean_grade):\n",
    "    \n",
    "    star_counts = np.array([per_of_4_stars, per_of_3_stars, per_of_2_stars, per_of_1_stars, per_of_undefined]) / 100\n",
    "    star_values = np.array([4, 3, 2, 1, 0])\n",
    "    variance = np.dot(star_counts, (star_values - mean_grade) ** 2)\n",
    "    return variance\n",
    "\n",
    "\n",
    "def get_5_class_labels(mean_grade):\n",
    "    \n",
    "    return round(mean_grade)\n",
    "    \n",
    "\n",
    "def get_9_class_labels(mean_grade):\n",
    "\n",
    "    return round(mean_grade * 2)\n",
    "\n",
    "\n",
    "df['Weighted Avg'] = df.apply(lambda row: calculate_weighted_average(row['4*'], row['3*'], row['2*'], row['1*'], row['Unclassified']), axis=1)\n",
    "\n",
    "df['Variance'] = df.apply(lambda row: calculate_variance(row['4*'], row['3*'], row['2*'], row['1*'], row['Unclassified'], row['Weighted Avg']), axis=1)\n",
    "\n",
    "\n",
    "df['5 Class Labels'] =  df.apply(lambda row: get_5_class_labels(row['Weighted Avg']), axis = 1)\n",
    "\n",
    "df['9 Class Labels'] =  df.apply(lambda row: get_9_class_labels(row['Weighted Avg']), axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"Institution name\", \"Unit of assessment name\", \"Number of doctoral degrees awarded in academic year 2013\", \"Number of doctoral degrees awarded in academic year 2014\", \"Number of doctoral degrees awarded in academic year 2015\", \"Number of doctoral degrees awarded in academic year 2016\", \"Number of doctoral degrees awarded in academic year 2017\", \"Number of doctoral degrees awarded in academic year 2018\", \"Number of doctoral degrees awarded in academic year 2019\"]\n",
    "\n",
    "ecs_selected_columns = ecs[selected_columns]\n",
    "\n",
    "merged_df = df.merge(ecs_selected_columns, on=[\"Institution name\", \"Unit of assessment name\"], how=\"left\")\n",
    "\n",
    "merged_df = merged_df[merged_df.index.isin(df.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "\n",
    "\n",
    "def remove_before_first_double_newline(text):\n",
    "    position = text.find('\\n\\n')\n",
    "    \n",
    "    if position != -1:\n",
    "        return text[position+2:]  # +2 to skip the '\\n\\n' itself\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def remove_html_tags(text):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Get the text without HTML tags\n",
    "    cleaned_text = soup.get_text()\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_markdown(text):\n",
    "    # Convert Markdown to HTML\n",
    "    html = markdown.markdown(text)\n",
    "    # Use BeautifulSoup to strip HTML tags\n",
    "    cleaned_text = remove_html_tags(html)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_html_and_markdown(text):\n",
    "    # First, remove Markdown formatting\n",
    "    text_without_markdown = remove_markdown(text)\n",
    "    # Then, remove any remaining HTML tags\n",
    "    cleaned_text = remove_html_tags(text_without_markdown)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_citations(text):\n",
    "    pattern = r'\\[.*?\\]|\\(.*?\\)'\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wordnet(penn_pos_tag):\n",
    "\n",
    "    # Function to convert Penn Treebank part-of-speed tags to corresponding WordNet\n",
    "    \n",
    "    tag_dictionary = {\n",
    "        'NN': 'n', 'NNS': 'n', 'NNP': 'n', 'NNPS': 'n',  \n",
    "        'JJ': 'a', 'JJR': 'a', 'JJS': 'a',               \n",
    "        'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v', \n",
    "        'RB': 'r', 'RBR': 'r', 'RBS': 'r',              \n",
    "        'MD': 'v'  }\n",
    "        \n",
    "    \n",
    "    # return the WordNet tag for given Peen Treebank tag, return \"n\" if none found\n",
    "    \n",
    "    try:\n",
    "        return tag_dictionary[penn_pos_tag[:2]]\n",
    "\n",
    "    except:\n",
    "        return \"n\"\n",
    "    \n",
    "\n",
    "stopwords_set = stopwords.words(\"english\")\n",
    "\n",
    "punctuation_set = set(punctuation)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "        \n",
    "    # account for edge case when text is empty \n",
    "        \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Convert all text to lowercase\n",
    "        \n",
    "    text = text.lower() \n",
    "        \n",
    "    # Remove characters that are repeated more than twice in a row\n",
    "        \n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "        \n",
    "    # Replace URLs with space \n",
    "        \n",
    "    text = re.sub(r\"http\\S+\",\" \", text)\n",
    "        \n",
    "    # Remove HTML tags from text\n",
    "        \n",
    "    text = re.sub(\"<.*?>\", \"\", text)\n",
    "        \n",
    "    # Ensure that there is space after each period\n",
    "        \n",
    "    text = re.sub(r'\\.(?![ .])', '. ', text)\n",
    "        \n",
    "    # Replace line break tags with space\n",
    "\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "\n",
    "    # Remove words containing digits\n",
    "        \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "    # Remove all numbers\n",
    "        \n",
    "    text = re.sub(r'\\b(?=.*\\d)(?=.*[a-zA-Z]).*?\\b', '', text)\n",
    "    \n",
    "    # Remove all numbers including floating points\n",
    "        \n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?', '', text)\n",
    "        \n",
    "    new_sentences = []\n",
    "        \n",
    "    #regexp tokenizer tokenizes sentences \n",
    "        \n",
    "    tokenized_sentences = [regexp_tokenize(sent, pattern = \"\\s+\", gaps = True) for sent in sent_tokenize(text)]\n",
    "\n",
    "    # Tokenize the cleaned text into sentences, then tokenize each sentence into words while removing spaces\n",
    "        \n",
    "    # Find pos tags for each sentence, pos_tag sents is faster than pos_tag \n",
    "        \n",
    "    new_sentences = []\n",
    "    \n",
    "    for sentences in pos_tag_sents(tokenized_sentences):\n",
    "        new_sentence = []\n",
    "        #iterate through word/tag pair\n",
    "        for word, tag in sentences:\n",
    "            # Check if word in punctuation and stopwords list\n",
    "            if word not in punctuation_set and word not in stopwords_set:\n",
    "                if word in contractions:\n",
    "                    # Expand contractions\n",
    "                    words = contractions[word].split(\" \")\n",
    "                    new_sentence = new_sentence + words\n",
    "                else:\n",
    "                    # Get WordNet Pos Tag and lemmatize word\n",
    "                    wordnet_pos = penn_to_wordnet(tag)\n",
    "                    lemmatized_word = wnl.lemmatize(word, pos = wordnet_pos)\n",
    "                    # Remove non-alphanumeric characters\n",
    "                        \n",
    "                    lemmatized_word = re.sub(r'[^\\w\\s]', '', lemmatized_word)\n",
    "                        \n",
    "                    #Check if lemmatized word exists and has length greater than 2\n",
    "                        \n",
    "                    if lemmatized_word and len(lemmatized_word) > 2:\n",
    "                        new_sentence.append(lemmatized_word)\n",
    "            \n",
    "        if new_sentence:\n",
    "            new_sentences.append(new_sentence)\n",
    "        \n",
    "    return new_sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = remove_before_first_double_newline(text)\n",
    "    \n",
    "    text = remove_html_and_markdown(text)\n",
    "    \n",
    "    text = remove_citations(text)\n",
    "    \n",
    "    text = preprocess_text(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "#merged_df[\"Summary of Impact Processed\"] = merged_df[\"1. Summary of the impact\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "merged_df[\"Details of Impact Processed\"] = merged_df[\"4. Details of the impact\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "merged_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sentences(sents_tokenized):\n",
    "\n",
    "    return ' '.join([' '.join(sentence) for sentence in sents_tokenized])\n",
    "\n",
    "\n",
    "merged_df[\"Details of Impact Processed Strings\"] = merged_df[\"Details of Impact Processed\"].apply(lambda x: join_sentences(x)) \n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mds-project",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
